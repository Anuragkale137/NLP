{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Using NLTk"
      ],
      "metadata": {
        "id": "79-EqmaAzKFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKhbtckby8Mv",
        "outputId": "ad68a82f-7033-4ae5-f007-65ee18a234fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word tokenize: ['Hello', 'world', '!', 'NLP', 'is', 'amazing', '.', 'Let', \"'s\", 'tokenize', 'this', 'sentence']\n",
            "sSentence tokenize: ['Hello world!', 'NLP is amazing .', \"Let's tokenize this sentence\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Hello world! NLP is amazing . Let's tokenize this sentence\"\n",
        "\n",
        "print(\"Word tokenize:\",word_tokenize(text))\n",
        "\n",
        "print(\"sSentence tokenize:\",sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Spacy"
      ],
      "metadata": {
        "id": "yku9-weQz-xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(text)\n",
        "print(\"word token:\",[token.text for token in doc])\n",
        "print(\"Sentence token:\",[sent.text for sent in doc.sents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko6AbIj0z0eY",
        "outputId": "8d39ed1e-cf16-49dc-bbe0-8f869f517eb2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word token: ['Hello', 'world', '!', 'NLP', 'is', 'amazing', '.', 'Let', \"'s\", 'tokenize', 'this', 'sentence']\n",
            "Sentence token: ['Hello world!', 'NLP is amazing .', \"Let's tokenize this sentence\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Tasks\n",
        "\n",
        "# Basic Word Count & Frequency Distribution"
      ],
      "metadata": {
        "id": "FHuUMgJH0ml3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "word = word_tokenize(text.lower())\n",
        "word_freq = Counter(word)\n",
        "print(\"Total word count:\",len(word))\n",
        "print(\"Word frequency distribution :\",word_freq.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Trks6oO0g5q",
        "outputId": "662d7dc9-c2fe-4488-9a47-59283b7e448d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total word count: 12\n",
            "Word frequency distribution : [('hello', 1), ('world', 1), ('!', 1), ('nlp', 1), ('is', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopword removal with frequency analysis"
      ],
      "metadata": {
        "id": "k0Y01qxL1g1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filterd_words = [word for word in word if word not in stop_words]\n",
        "filterd_freq = Counter(filterd_words)\n",
        "\n",
        "print(\"Filterd word frequency distribution:\", filterd_freq.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVIkt_RS1MwU",
        "outputId": "b488e460-31c7-4737-a3e9-25ab5b3f6ad7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filterd word frequency distribution: [('hello', 1), ('world', 1), ('!', 1), ('nlp', 1), ('amazing', 1)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER with Context Analysis"
      ],
      "metadata": {
        "id": "cWwcLwQ22u3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\") # load spacy model\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "entity_freq = Counter([ent.text for ent in doc.ents])\n",
        "for ent,freq in entity_freq.most_common():\n",
        "  print(f\"Entity:{ent},count:{freq},Lable:{nlp(ent)[0].ent_type}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krtVtgUF2aLQ",
        "outputId": "a7194391-9bbf-4196-8e5a-fc1da2bb8d9b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity:Apple,count:1,Lable:383\n",
            "Entity:U.K.,count:1,Lable:384\n",
            "Entity:$1 billion,count:1,Lable:394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Grams with frequency analysis"
      ],
      "metadata": {
        "id": "-GclTQEO33Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "bigrams = list(ngrams(word,2))\n",
        "trigrams = list(ngrams(word,3))\n",
        "bigram_freq = Counter(bigrams)\n",
        "trigram_freq = Counter(trigrams)\n",
        "\n",
        "print(\"Most common bigrams:\",bigram_freq.most_common(5))\n",
        "print(\"Most common trigrams:\",trigram_freq.most_common(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xK9_QiJ3mCM",
        "outputId": "4991ed73-a48e-4a49-f8e0-4c506b4a0fad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common bigrams: [(('hello', 'world'), 1), (('world', '!'), 1), (('!', 'nlp'), 1), (('nlp', 'is'), 1), (('is', 'amazing'), 1)]\n",
            "Most common trigrams: [(('hello', 'world', '!'), 1), (('world', '!', 'nlp'), 1), (('!', 'nlp', 'is'), 1), (('nlp', 'is', 'amazing'), 1), (('is', 'amazing', '.'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keyword Extraction using TF-IDF with Tokenization"
      ],
      "metadata": {
        "id": "BxijDH-b4u1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform([text])\n",
        "feature_array = vectorizer.get_feature_names_out()\n",
        "importance = X.toarray().flatten()\n",
        "important_words = sorted(zip(feature_array, importance), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(\"Top Keywords:\", [word for word, _ in important_words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1wB4VLU4gzH",
        "outputId": "5fd09569-341d-4e7d-926b-35487e982c9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Keywords: ['apple', 'at', 'billion', 'buying', 'for']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging"
      ],
      "metadata": {
        "id": "pIYCsrde764D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(f\"Token: {token.text},POS:{token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyLk_HFY6HzC",
        "outputId": "e3f0399b-02bb-4609-a081-0ec4a0522ef2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: Apple,POS:PROPN\n",
            "Token: is,POS:AUX\n",
            "Token: looking,POS:VERB\n",
            "Token: at,POS:ADP\n",
            "Token: buying,POS:VERB\n",
            "Token: U.K.,POS:PROPN\n",
            "Token: startup,POS:VERB\n",
            "Token: for,POS:ADP\n",
            "Token: $,POS:SYM\n",
            "Token: 1,POS:NUM\n",
            "Token: billion,POS:NUM\n",
            "Token: .,POS:PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence similarity"
      ],
      "metadata": {
        "id": "msr0fwYH88kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "sentence = [\"I love NLP\", \"NLP is great for text processing\", \"Machine learning is amazing\"]\n",
        "vectorizeer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(sentence)\n",
        "print(\"Sentence Similarity Matrix:\")\n",
        "print((x*x.T).toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNNKhqvE8y-N",
        "outputId": "32c962f7-d67b-4b76-9db0-430180f21651"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Similarity Matrix:\n",
            "[[1.         0.20273527 0.        ]\n",
            " [0.20273527 1.         0.13464597]\n",
            " [0.         0.13464597 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization"
      ],
      "metadata": {
        "id": "1F8mPre79uiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_score = {}\n",
        "for sent in doc.sents:\n",
        "  for word in word_tokenize(sent.text.lower()):\n",
        "    if word in filterd_freq:\n",
        "      sentence_score[sent.text] = sentence_score.get(sent.text,0) + filterd_freq[word]\n",
        "sorted_sentences = sorted(sentence_score,key = sentence_score.get,reverse=True)\n",
        "summary = \" \".join(sorted_sentences[:2])\n",
        "print(\"Summary:\",summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M5LOkqg9gIr",
        "outputId": "0495ae29-b0d5-451e-d3a1-e8f9a8ae8fa6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Apple is looking at buying U.K. startup for $1 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download required NLTK tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"Natural Language Processing (NLP) is a field of AI that enables machines to understand human language.\n",
        "It is used in chatbots, sentiment analysis, and language translation.\n",
        "One of the key challenges in NLP is understanding context and ambiguity in sentences.\n",
        "Deep learning models like transformers have significantly improved NLP applications.\"\"\"\n",
        "\n",
        "# Process text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenize words and compute frequency\n",
        "words = [word.lower() for word in word_tokenize(text) if word.isalnum()]\n",
        "filtered_freq = Counter(words)\n",
        "\n",
        "# Compute sentence scores\n",
        "sentence_scores = {}\n",
        "for sent in doc.sents:\n",
        "    for word in word_tokenize(sent.text.lower()):\n",
        "        if word in filtered_freq:\n",
        "            sentence_scores[sent.text] = sentence_scores.get(sent.text, 0) + filtered_freq[word]\n",
        "\n",
        "# Sort sentences by score\n",
        "sorted_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)\n",
        "\n",
        "# Select top-ranked sentences for summary\n",
        "summary = \" \".join(sorted_sentences[:2])\n",
        "\n",
        "# Print summary\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEq9lYs--tS8",
        "outputId": "28d24369-5494-4826-8a1f-500e3e79f529"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: Natural Language Processing (NLP) is a field of AI that enables machines to understand human language.\n",
            " One of the key challenges in NLP is understanding context and ambiguity in sentences.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KFo4Aw2n_ZgP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}