{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Python Built-in-Function - Count Words"
      ],
      "metadata": {
        "id": "bc4lPDWFDvvg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkGmC9nyDnkZ",
        "outputId": "2f4cbda2-8f8c-439e-f51b-d704914377e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count :  8\n"
          ]
        }
      ],
      "source": [
        "text = \"Natural Language Processing (NLP) is a fascinating field!\"\n",
        "words = text.split()\n",
        "word_count = len(words)\n",
        "print(\"Word Count : \",word_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex - Handle Punctuation and Count Words"
      ],
      "metadata": {
        "id": "AL7T2w1IEMo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Text\n",
        "import re\n",
        "def count_word(text):\n",
        "  word = re.findall(r'\\b\\w+\\b',text)\n",
        "  return len(word)\n",
        "\n",
        "text = \"Hello, world! Welcome to NLP.\"\n",
        "print(\"Word Count:\",count_word(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mRlGNT3EHDk",
        "outputId": "5a130405-7e0b-49af-9cd8-a08a51ef52a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTk-Python"
      ],
      "metadata": {
        "id": "mJm0UdqhE3u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk import word_tokenize\n",
        "\n",
        "def count_word_nltk(text):\n",
        "  words = word_tokenize(text)\n",
        "  return len(words)\n",
        "\n",
        "text = \"Let's explore NLP with Python!\"\n",
        "print(\"Word Count : \",count_word_nltk(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9PZ2RGtE1Cp",
        "outputId": "cb088d42-91e9-469c-85d7-bb22dba37403"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count :  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy"
      ],
      "metadata": {
        "id": "5Xf507dOFtkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def count_words_spacy(text):\n",
        "  doc = nlp(text)\n",
        "  words = [token.text for token in doc if not token.is_punct]\n",
        "  return len(words)\n",
        "\n",
        "text = \"SpaCy is a powerful NLP library.\"\n",
        "print(\"Word Count : \",count_words_spacy(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95SG2tSQFfp4",
        "outputId": "9bec5b0f-c97f-49da-b442-843b2cddffd7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count :  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counter"
      ],
      "metadata": {
        "id": "sU3bO_s1GaPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def word_frequency(text):\n",
        "  words = text.lower().split()\n",
        "  return Counter(words)\n",
        "\n",
        "text = \"NLP is fun. NLP is powerful. NLP is the future.\"\n",
        "print(\"Word Frequencies:\", word_frequency(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F48_AvcxGR2y",
        "outputId": "4d2358b1-2254-4c08-91cb-dc4993d33be2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequencies: Counter({'nlp': 3, 'is': 3, 'fun.': 1, 'powerful.': 1, 'the': 1, 'future.': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Task 1 - Short vs Long sentences"
      ],
      "metadata": {
        "id": "vVxSX0DtG-0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def categorize_sentences(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  categorized = {\"short\":[],\"long\":[]}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    word_count = len(word_tokenize(sentence))\n",
        "    if word_count < 5:\n",
        "      categorized[\"short\"].append(sentence)\n",
        "    else:\n",
        "      categorized[\"long\"].append(sentence)\n",
        "\n",
        "  return categorized\n",
        "\n",
        "text = \"Hello! NLP is fun. Let's learn NLP together. It helps in many fields.\"\n",
        "categories = categorize_sentences(text)\n",
        "print(\"Short Sentence :\",categories[\"short\"])\n",
        "print(\"Long Sentence :\",categories[\"long\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZb0QE88GxTa",
        "outputId": "320d361d-8b92-4a45-d0a6-5c73e4cf9ad8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short Sentence : ['Hello!', 'NLP is fun.']\n",
            "Long Sentence : [\"Let's learn NLP together.\", 'It helps in many fields.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Task 2 - Rule Based Sentiment Analysis"
      ],
      "metadata": {
        "id": "Wztzp7w9IkMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "positive_word = {\"happy\", \"good\", \"great\", \"excellent\", \"love\", \"amazing\"}\n",
        "negative_word = {\"sad\", \"bad\", \"terrible\", \"hate\", \"awful\", \"worst\"}\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "  start_time = time.time()\n",
        "  words = text.lower().split()\n",
        "  pos_count = sum(1 for word in words if word in positive_word)\n",
        "  neg_count = sum(1 for word in words if word in negative_word)\n",
        "\n",
        "  sentiment = \"Positive Sentiment \" if pos_count > neg_count else \"Negative Sentiment\" if neg_count > pos_count else \"Neutral Sentiment\"\n",
        "\n",
        "  end_time = time.time()\n",
        "  execution_time = end_time - start_time\n",
        "  return sentiment,execution_time\n",
        "\n",
        "text = \"\"\"\n",
        "I'm happy you're the President, and thank you for bringing me home.\n",
        "I have never been so proud to be an American citizen. Thank you, Mr. President.\n",
        "\"\"\"\n",
        "sentiment , exec_time = sentiment_analysis(text)\n",
        "print(\"Sentiment :\",sentiment)\n",
        "print(\"Execution time: {:.6f} seconds\".format(exec_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgDkmH-ZIbm1",
        "outputId": "dcf50d70-efe4-4fd5-8171-3eb21df51d31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment : Positive Sentiment \n",
            "Execution time: 0.000012 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IaZgPrWoMlFU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}